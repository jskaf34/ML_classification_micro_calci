{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Techniques of Artificial Intelligence Project** : \n",
    "## Project 3 : Classification of benignant/malignant calcification in the context of breast cancer\n",
    "SKAF Joey                                                           \n",
    "Student number : 0595931"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breast cancer is unfortunatly a common cancer that impacts women. According to the World Health Organization, in 2020, there were 2.3 million women diagnosed with breast cancer and 685 000 deaths globally.\n",
    "Nonethless by an early diagnostic of the disease, breast cancer can be treated and cured. Prediciting it by analysing symptoms is thus crucial. One way to do that is by analysing micro-calcification in women's breast : their shape and texture properties of individual micros allow to predict malignancy of a tumor. \n",
    "Thanks to Machine Learning techniques, given a dataset with different features of the malcro-calcification, we can try to predict if a women has cancer or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we have is composed of 3561 micro-calcifications of 96 patients that has/had cancer or not (specified in the last column with 0 : no cancer and 1 : cancer). Each micro-calcification has as first feature, the patient on who we observed it, then 150 different features measurable, and last but not least, a label indicating if the patient has/had cancer or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Presentation of the approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to solve this task in two steps :\n",
    "\n",
    "1. Because there is a strong correlation between micro-calcification and breast cancer, we can say that having cancer implies having malignant macro-calcification but because we assume all micros have the same label, having breast cancer implies that all the macros are malignant. And the same for the equivalence here : no-breast cancer <=> all micros are benign. So, we will do a supervised learning, saying that label \"breast cancer\" is equivalent, in our problem, to \"malignant/benign macro-calcification\". \n",
    "\n",
    "2. Given the learning algorithm we acquired in the first step, we can classify the micros and give them a label (even if the label is wrong), we do not have anymore the hypothesis of all micros having the same label.\n",
    "So we will add as a feature of the calcification, the column \"malignant/benign macro-calcification\", and based on that, we will do a supervised learning, with as the supervised label, the \"breast cancer\" column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Presentation of the technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try here to test 4 different models to achieve the classification :\n",
    "\n",
    "1. the Logistic Regression algorithm on a dimensionally reduced dataset (we use the Principal Component Analysis method to do so). PCA can be a good solution \n",
    "2. the Support Vector Machine algorithm\n",
    "3. a Neural Network \n",
    "4. a Neural Network on a dimensionally reduced dataset (i.e applying PCA on the dataset)\n",
    "\n",
    "Because the dataset is unbalanced (see bellow), we will use as an error metric the F1 score that is adapted to unbalanced dataset by considering the harmonic mean of precision and recall error meatrics. Nevertheless, we need to take into account the false negatives case to choose the model. It corresponds to the case where the patient has/had cancer, but the model predicts it as \"no cancer\". Thus, we also calculate the number of false negatives case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing some librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! A EXECUTER UNE FOIS !!!\n",
    "raw_data = pd.read_excel(\"data.xlsx\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature: 152\n",
      "Number of micro-calcification: 3562\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of feature:\",len(raw_data[0]))\n",
    "print(\"Number of micro-calcification:\", len(raw_data[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating our data from the label needed for supervised learning. We also discard the feature \"patient\" that won't be usefull in our study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = raw_data[:,1:(len(raw_data[0])-1)], raw_data[:,len(raw_data[0])-1:len(raw_data[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we split the data in equal proportion between label 1 and 0 to not biais our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n",
      "1542\n"
     ]
    }
   ],
   "source": [
    "X_label_0,y_label_0 = X[:2020],y[:2020]\n",
    "X_label_1,y_label_1 = X[2020:],y[2020:]\n",
    "\n",
    "print(len(y_label_0))\n",
    "print(len(y_label_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the labels are unbalanced between 0s and 1s. We notice this unbalance as a potential biais for our dataset so we choose as an error metric the f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split into training-validating set and test set our data for label 0s and 1s separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "X_train_0, X_test_0, y_train_0, y_test_0 = train_test_split(X_label_0, y_label_0, test_size=0.025)\n",
    "print(len(X_train_0))\n",
    "print(len(X_test_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1503\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_label_1, y_label_1, test_size=0.025)\n",
    "print(len(X_train_1))\n",
    "print(len(X_test_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the labels to have same proportion of 0s and 1s in training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3472\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "X_test,y_test = np.concatenate([X_test_0,X_test_1]),np.concatenate([y_test_0,y_test_1])\n",
    "X_train, y_train= np.concatenate([X_train_0,X_train_1]),np.concatenate([y_train_0,y_train_1])\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshuffle everything to have a good dataset for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indexes = np.arange(len(X_train))\n",
    "np.random.shuffle(train_indexes)\n",
    "X_train = X_train[train_indexes]\n",
    "y_train = y_train[train_indexes]\n",
    "\n",
    "test_indexes = np.arange(len(X_test))\n",
    "np.random.shuffle(test_indexes)\n",
    "X_test = X_test[test_indexes]\n",
    "y_test = y_test[test_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the tools to center and reduce the data, plus the PCA for the first and the fourth techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We center and reduce our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.04504505e-01, -2.33066898e-01, -3.54485248e-01, ...,\n",
       "         3.71720470e-01, -2.32587732e-01,  1.34990179e+00],\n",
       "       [-3.81408105e-02, -7.54843247e-02, -2.85978183e-01, ...,\n",
       "        -1.08423004e+00, -2.74558684e-01, -7.31260297e-01],\n",
       "       [ 2.56634790e-01, -1.00437840e+00, -1.07254141e+00, ...,\n",
       "        -6.98094048e-01, -7.20579346e-01, -5.74988353e-01],\n",
       "       ...,\n",
       "       [ 6.84225334e-01,  1.21647855e-01, -3.36565768e-04, ...,\n",
       "         1.00898502e-01,  1.00384145e+00, -2.01344891e-01],\n",
       "       [ 6.26355939e-01,  6.42151512e-01,  4.16013453e-01, ...,\n",
       "         1.40181842e+00,  1.15428551e+00,  1.13150543e+00],\n",
       "       [ 1.61380349e+00,  2.68972629e+00, -1.49471390e+00, ...,\n",
       "        -1.07335416e+00,  3.14766809e-01, -5.64328362e-01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_standardized = scaler.transform(X_train)\n",
    "X_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do dimensional reducing to our dataset through the PCA technique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-12.35865328,  -2.38248564,   4.48287003, ...,   1.12712305,\n",
       "          0.46651949,  -0.28492952],\n",
       "       [  7.14758454,  -2.73631845,  -1.2845036 , ...,  -0.52552937,\n",
       "          0.07613967,   0.19432478],\n",
       "       [ -7.11700355,  -3.57085625,   1.6380319 , ...,  -0.02668062,\n",
       "          0.96359428,  -0.6575074 ],\n",
       "       ...,\n",
       "       [  0.28911274,   4.43477764,  -3.22315701, ...,  -0.26068796,\n",
       "         -0.47930749,   0.23702105],\n",
       "       [ -7.60899869,   3.37293436,  -0.455115  , ...,  -0.72704821,\n",
       "          0.39280078,  -0.29810586],\n",
       "       [ 19.866319  ,   0.35883619,   9.64466459, ...,   0.70139385,\n",
       "         -1.58196588,  -0.27437916]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(0.9)\n",
    "pca.fit(X_standardized)\n",
    "X_transformed = pca.transform(X_standardized)\n",
    "X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see how much information each axe found by the PCA has to summarize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44787719 0.11487575 0.0559034  0.04192724 0.02899031 0.02650838\n",
      " 0.02479033 0.0195765  0.01524052 0.01292839 0.01262256 0.01137507\n",
      " 0.00980236 0.00885644 0.00811045 0.00782032 0.0064192  0.00623578\n",
      " 0.00614093 0.00553254 0.00535789 0.00526537 0.00488269 0.00455471\n",
      " 0.0044519  0.00429625]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to do a cross-validation on our training set, regarding the small number of data that we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "kf = KFold(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First model : the Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "models_scores_val_log_reg = []\n",
    "\n",
    "\n",
    "for train_indexes, val_indexes in kf.split(X_transformed):\n",
    "    model_temp = LogisticRegression()\n",
    "    model_temp.fit(X_transformed[train_indexes], y_train[train_indexes].ravel())\n",
    "    y_pred_temp = model_temp.predict(X_transformed[val_indexes])\n",
    "    f1_score_temp = f1_score(y_train[val_indexes],y_pred_temp)\n",
    "    y_folds = y_train[val_indexes]\n",
    "    score_false_negativ = 0\n",
    "    nb_positivs = 0\n",
    "    for i in range(len(y_train[val_indexes])):\n",
    "        if y_folds[i][0]:\n",
    "            nb_positivs+=1\n",
    "            if not(y_pred_temp[i]):\n",
    "                score_false_negativ+=1\n",
    "    models_scores_val_log_reg.append([model_temp,f1_score_temp,score_false_negativ/nb_positivs])\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different models given by the cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[LogisticRegression(), 0.7251461988304092, 0.3333333333333333],\n",
       " [LogisticRegression(), 0.6818181818181819, 0.4],\n",
       " [LogisticRegression(), 0.6593406593406594, 0.4],\n",
       " [LogisticRegression(), 0.6330935251798562, 0.42857142857142855],\n",
       " [LogisticRegression(), 0.601156069364162, 0.4090909090909091],\n",
       " [LogisticRegression(), 0.5750000000000001, 0.43209876543209874],\n",
       " [LogisticRegression(), 0.6741573033707865, 0.38144329896907214],\n",
       " [LogisticRegression(), 0.7272727272727272, 0.2967032967032967],\n",
       " [LogisticRegression(), 0.711864406779661, 0.37],\n",
       " [LogisticRegression(), 0.7395833333333333, 0.297029702970297],\n",
       " [LogisticRegression(), 0.6144578313253013, 0.4069767441860465],\n",
       " [LogisticRegression(), 0.632183908045977, 0.47115384615384615],\n",
       " [LogisticRegression(), 0.5986394557823129, 0.4883720930232558],\n",
       " [LogisticRegression(), 0.6198830409356725, 0.47],\n",
       " [LogisticRegression(), 0.7282051282051282, 0.3238095238095238],\n",
       " [LogisticRegression(), 0.6666666666666666, 0.3829787234042553]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_scores_val_log_reg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the model with the best f1 score given the lowest number of false negativ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7395833333333333\n",
      "Number of false negatives: 0.297029702970297\n"
     ]
    }
   ],
   "source": [
    "model_log_rec_selected = models_scores_val_log_reg[-7][0]\n",
    "print(\"F1 score:\",models_scores_val_log_reg[-7][1])\n",
    "print(\"Number of false negatives:\",models_scores_val_log_reg[-7][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second model, SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "models_scores_val_SVC = []\n",
    "\n",
    "\n",
    "for train_indexes, val_indexes in kf.split(X_standardized):\n",
    "    model_temp = SVC(gamma='auto')\n",
    "    model_temp.fit(X_standardized[train_indexes], y_train[train_indexes].ravel())\n",
    "    y_pred_temp = model_temp.predict(X_standardized[val_indexes])\n",
    "    f1_score_temp = f1_score(y_train[val_indexes],y_pred_temp)\n",
    "    y_folds = y_train[val_indexes]\n",
    "    score_false_negativ = 0\n",
    "    nb_positivs=0\n",
    "    for i in range(len(y_train[val_indexes])):\n",
    "        if y_folds[i][0]:\n",
    "            nb_positivs+=1\n",
    "            if not(y_pred_temp[i]):\n",
    "                score_false_negativ+=1\n",
    "    models_scores_val_SVC.append([model_temp,f1_score_temp,score_false_negativ/nb_positivs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[SVC(gamma='auto'), 0.7560975609756097, 0.3333333333333333],\n",
       " [SVC(gamma='auto'), 0.6857142857142857, 0.4],\n",
       " [SVC(gamma='auto'), 0.7934782608695652, 0.27],\n",
       " [SVC(gamma='auto'), 0.6917293233082706, 0.4025974025974026],\n",
       " [SVC(gamma='auto'), 0.6583850931677019, 0.3977272727272727],\n",
       " [SVC(gamma='auto'), 0.6527777777777778, 0.41975308641975306],\n",
       " [SVC(gamma='auto'), 0.6863905325443788, 0.4020618556701031],\n",
       " [SVC(gamma='auto'), 0.7904191616766467, 0.27472527472527475],\n",
       " [SVC(gamma='auto'), 0.7251461988304093, 0.38],\n",
       " [SVC(gamma='auto'), 0.7675675675675676, 0.297029702970297],\n",
       " [SVC(gamma='auto'), 0.6712328767123289, 0.43023255813953487],\n",
       " [SVC(gamma='auto'), 0.6227544910179641, 0.5],\n",
       " [SVC(gamma='auto'), 0.6619718309859154, 0.45348837209302323],\n",
       " [SVC(gamma='auto'), 0.650887573964497, 0.45],\n",
       " [SVC(gamma='auto'), 0.6927374301675978, 0.4095238095238095],\n",
       " [SVC(gamma='auto'), 0.6625, 0.43617021276595747]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_scores_val_SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7934782608695652\n",
      "Number of false negatives: 0.27\n"
     ]
    }
   ],
   "source": [
    "model_svm_selected = models_scores_val_SVC[2][0]\n",
    "print(\"F1 score:\",models_scores_val_SVC[2][1])\n",
    "print(\"Number of false negatives:\",models_scores_val_SVC[2][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third model, Neural Network (without PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "models_scores_val_NN = []\n",
    "\n",
    "\n",
    "for train_indexes, val_indexes in kf.split(X_standardized):\n",
    "    model_temp = MLPClassifier(hidden_layer_sizes=(6,10,6,4), activation=\"relu\",max_iter=1000)\n",
    "    model_temp.fit(X_standardized[train_indexes], y_train[train_indexes].ravel())\n",
    "    y_pred_temp = model_temp.predict(X_standardized[val_indexes])\n",
    "    f1_score_temp = f1_score(y_train[val_indexes],y_pred_temp)\n",
    "    y_folds = y_train[val_indexes]\n",
    "    score_false_negativ = 0\n",
    "    nb_positivs = 0\n",
    "    for i in range(len(y_train[val_indexes])):\n",
    "        if y_folds[i][0]:\n",
    "            nb_positivs+=1\n",
    "            if not(y_pred_temp[i]):\n",
    "                score_false_negativ+=1\n",
    "    models_scores_val_NN.append([model_temp,f1_score_temp,score_false_negativ/nb_positivs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7474747474747475,\n",
       "  0.20430107526881722],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6888888888888889,\n",
       "  0.38],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.743455497382199,\n",
       "  0.29],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6853146853146853,\n",
       "  0.36363636363636365],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.625,\n",
       "  0.375],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6405228758169934,\n",
       "  0.3950617283950617],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7011494252873565,\n",
       "  0.3711340206185567],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7630057803468209,\n",
       "  0.27472527472527475],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6526315789473685,\n",
       "  0.38],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7499999999999999,\n",
       "  0.2871287128712871],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6225165562913907,\n",
       "  0.45348837209302323],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.641304347826087,\n",
       "  0.4326923076923077],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6543209876543209,\n",
       "  0.38372093023255816],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7040816326530612,\n",
       "  0.31],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7027027027027026,\n",
       "  0.38095238095238093],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6844919786096257,\n",
       "  0.3191489361702128]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_scores_val_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could select the one doing the best in terms of f1 score but here we prefer selecting the one doing the less bad on numbers of false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7474747474747475\n",
      "Number of false negatives: 0.20430107526881722\n"
     ]
    }
   ],
   "source": [
    "model_NN_selected = models_scores_val_NN[0][0]\n",
    "print(\"F1 score:\",models_scores_val_NN[0][1])\n",
    "print(\"Number of false negatives:\",models_scores_val_NN[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth model, NN with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "models_scores_val_NN_PCA = []\n",
    "\n",
    "\n",
    "for train_indexes, val_indexes in kf.split(X_transformed):\n",
    "    model_temp = MLPClassifier(hidden_layer_sizes=(6,10,6,4), activation=\"relu\",max_iter=1000)\n",
    "    model_temp.fit(X_transformed[train_indexes], y_train[train_indexes].ravel())\n",
    "    y_pred_temp = model_temp.predict(X_transformed[val_indexes])\n",
    "    f1_score_temp = f1_score(y_train[val_indexes],y_pred_temp)\n",
    "    y_folds = y_train[val_indexes]\n",
    "    score_false_negativ = 0\n",
    "    nb_positivs = 0\n",
    "    for i in range(len(y_train[val_indexes])):\n",
    "        if y_folds[i][0]:\n",
    "            nb_positivs+=1\n",
    "            if not(y_pred_temp[i]):\n",
    "                score_false_negativ+=1\n",
    "    models_scores_val_NN_PCA.append([model_temp,f1_score_temp,score_false_negativ/nb_positivs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7500000000000001,\n",
       "  0.2903225806451613],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6547619047619048,\n",
       "  0.45],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7708333333333333,\n",
       "  0.26],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6808510638297872,\n",
       "  0.37662337662337664],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6467065868263473,\n",
       "  0.38636363636363635],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6103896103896104,\n",
       "  0.41975308641975306],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6867469879518072,\n",
       "  0.41237113402061853],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7701149425287357,\n",
       "  0.26373626373626374],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6885245901639344,\n",
       "  0.37],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7623762376237624,\n",
       "  0.2376237623762376],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6335403726708074,\n",
       "  0.4069767441860465],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6432748538011696,\n",
       "  0.47115384615384615],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6490066225165563,\n",
       "  0.43023255813953487],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6400000000000001,\n",
       "  0.44],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6984126984126985,\n",
       "  0.37142857142857144],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6626506024096385,\n",
       "  0.4148936170212766]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_scores_val_NN_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7708333333333333\n",
      "Number of false negatives: 0.26\n"
     ]
    }
   ],
   "source": [
    "model_NN_PCA_selected = models_scores_val_NN_PCA[2][0]\n",
    "print(\"F1 score:\",models_scores_val_NN_PCA[2][1])\n",
    "print(\"Number of false negatives:\",models_scores_val_NN_PCA[2][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final test, we test our models on the test data set, we center and reduce it with the mean and the standard deviation of the training data set and we project it for the models that needed it onto the subspace found thanks to PCA on the training dataset\n",
    "Then we evaluate the performance of our models on the test set thanks to the f1 score and the number of false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score for Logistic Regression with PCA transformation on X_test: 0.6176470588235294 and the number of false negativ: 18\n",
      "f1_score for SVM on X_test: 0.6451612903225806 and the number of false negativ: 19\n",
      "f1_score for NN on X_test: 0.6233766233766234 and the number of false negativ: 15\n",
      "f1_score for NN with PCA transformation on X_test: 0.6285714285714286 and the number of false negativ: 17\n"
     ]
    }
   ],
   "source": [
    "X_test_standardized = scaler.transform(X_test)\n",
    "X_test_transformed = pca.transform(X_test_standardized)\n",
    "\n",
    "y_pred_X_test_log_rec = model_log_rec_selected.predict(X_test_transformed)\n",
    "y_pred_X_test_SVM = model_svm_selected.predict(X_test_standardized)\n",
    "y_pred_X_test_NN_PCA = model_NN_PCA_selected.predict(X_test_transformed)\n",
    "y_pred_X_test_NN = model_NN_selected.predict(X_test_standardized)\n",
    "\n",
    "f1_score_log_rec = f1_score(y_test.ravel(),y_pred_X_test_log_rec)\n",
    "score_false_negativ_log_rec = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] and not(y_pred_X_test_log_rec[i]):\n",
    "        score_false_negativ_log_rec+=1\n",
    "\n",
    "f1_score_SVM = f1_score(y_test.ravel(),y_pred_X_test_SVM)\n",
    "score_false_negativ_SVM = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] and not(y_pred_X_test_SVM[i]):\n",
    "        score_false_negativ_SVM+=1\n",
    "\n",
    "f1_score_NN = f1_score(y_test.ravel(),y_pred_X_test_NN)\n",
    "score_false_negativ_NN = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] and not(y_pred_X_test_NN[i]):\n",
    "        score_false_negativ_NN+=1\n",
    "\n",
    "f1_score_NN_PCA = f1_score(y_test.ravel(),y_pred_X_test_NN_PCA)\n",
    "score_false_negativ_NN_PCA = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] and not(y_pred_X_test_NN_PCA[i]):\n",
    "        score_false_negativ_NN_PCA+=1\n",
    "\n",
    "\n",
    "\n",
    "print(f\"f1_score for Logistic Regression with PCA transformation on X_test: {f1_score_log_rec} and the number of false negativ: {score_false_negativ_log_rec}\")\n",
    "print(f\"f1_score for SVM on X_test: {f1_score_SVM} and the number of false negativ: {score_false_negativ_SVM}\")\n",
    "print(f\"f1_score for NN on X_test: {f1_score_NN} and the number of false negativ: {score_false_negativ_NN}\")\n",
    "print(f\"f1_score for NN with PCA transformation on X_test: {f1_score_NN_PCA} and the number of false negativ: {score_false_negativ_NN_PCA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN with PCA seems here to be a good model to deal with our database, with the second highest f1 score and the second smallest number of false negatives, which is a good tradeoff. I choose to focus on this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go through the same process than above, using the 4 same models but trained on the new dataset made of the old X on which we added the column \"benignant/malignant calcification\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_malignant = model_NN_PCA_selected.predict(pca.transform(scaler.transform(X)))\n",
    "X_new = np.hstack([X,np.atleast_2d(col_malignant).T])\n",
    "X_new_label_0 = X_new[:2020]\n",
    "X_new_label_1 = X_new[2020:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new_0, X_test_new_0, y_train_new_0, y_test_new_0 = train_test_split(X_new_label_0, y_label_0, test_size=0.025)\n",
    "X_train_new_1, X_test_new_1, y_train_new_1, y_test_new_1 = train_test_split(X_new_label_1, y_label_1, test_size=0.025)\n",
    "X_test_new,y_test_new = np.concatenate([X_test_new_0,X_test_new_1]),np.concatenate([y_test_new_0,y_test_new_1])\n",
    "X_train_new, y_train_new= np.concatenate([X_train_new_0,X_train_new_1]),np.concatenate([y_train_new_0,y_train_new_1])\n",
    "train_indexes_new = np.arange(len(X_train_new))\n",
    "np.random.shuffle(train_indexes_new)\n",
    "X_train_new = X_train_new[train_indexes_new]\n",
    "y_train_new = y_train_new[train_indexes_new]\n",
    "\n",
    "test_indexes_new = np.arange(len(X_test_new))\n",
    "np.random.shuffle(test_indexes_new)\n",
    "X_test_new = X_test_new[test_indexes_new]\n",
    "y_test_new = y_test_new[test_indexes_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_2 = StandardScaler()\n",
    "scaler_2.fit(X_train_new)\n",
    "X_new_standardized = scaler_2.transform(X_train_new)\n",
    "pca_2 = PCA(0.9)\n",
    "pca_2.fit(X_new_standardized)\n",
    "X_new_transformed = pca_2.transform(X_new_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_scores_val_log_reg_new = []\n",
    "\n",
    "\n",
    "for train_indexes, val_indexes in kf.split(X_new_transformed):\n",
    "    model_temp = LogisticRegression()\n",
    "    model_temp.fit(X_new_transformed[train_indexes], y_train_new[train_indexes].ravel())\n",
    "    y_pred_temp = model_temp.predict(X_new_transformed[val_indexes])\n",
    "    f1_score_temp = f1_score(y_train_new[val_indexes],y_pred_temp)\n",
    "    y_folds = y_train_new[val_indexes]\n",
    "    score_false_negativ = 0\n",
    "    nb_positivs = 0\n",
    "    for i in range(len(y_train[val_indexes])):\n",
    "        if y_folds[i][0]:\n",
    "            nb_positivs+=1\n",
    "            if not(y_pred_temp[i]):\n",
    "                score_false_negativ+=1\n",
    "    models_scores_val_log_reg_new.append([model_temp,f1_score_temp,score_false_negativ/nb_positivs])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[LogisticRegression(), 0.776470588235294, 0.2826086956521739],\n",
       " [LogisticRegression(), 0.6628571428571428, 0.41414141414141414],\n",
       " [LogisticRegression(), 0.738095238095238, 0.30337078651685395],\n",
       " [LogisticRegression(), 0.6716417910447761, 0.4155844155844156],\n",
       " [LogisticRegression(), 0.75, 0.30303030303030304],\n",
       " [LogisticRegression(), 0.7675675675675675, 0.2828282828282828],\n",
       " [LogisticRegression(), 0.7362637362637363, 0.33],\n",
       " [LogisticRegression(), 0.7329192546583853, 0.2716049382716049],\n",
       " [LogisticRegression(), 0.7692307692307692, 0.3010752688172043],\n",
       " [LogisticRegression(), 0.728476821192053, 0.32926829268292684],\n",
       " [LogisticRegression(), 0.7560975609756098, 0.29545454545454547],\n",
       " [LogisticRegression(), 0.7513227513227513, 0.30392156862745096],\n",
       " [LogisticRegression(), 0.7676767676767676, 0.2962962962962963],\n",
       " [LogisticRegression(), 0.7484662576687118, 0.3146067415730337],\n",
       " [LogisticRegression(), 0.7771428571428571, 0.31313131313131315],\n",
       " [LogisticRegression(), 0.7772020725388601, 0.29245283018867924]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_scores_val_log_reg_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.776470588235294\n",
      "Number of false negatives: 0.2826086956521739\n"
     ]
    }
   ],
   "source": [
    "model_log_rec_selected_new = models_scores_val_log_reg_new[0][0]\n",
    "print(\"F1 score:\",models_scores_val_log_reg_new[0][1])\n",
    "print(\"Number of false negatives:\",models_scores_val_log_reg_new[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_scores_val_SVC_new = []\n",
    "\n",
    "\n",
    "for train_indexes, val_indexes in kf.split(X_new_standardized):\n",
    "    model_temp = SVC(gamma='auto')\n",
    "    model_temp.fit(X_new_standardized[train_indexes], y_train_new[train_indexes].ravel())\n",
    "    y_pred_temp = model_temp.predict(X_new_standardized[val_indexes])\n",
    "    f1_score_temp = f1_score(y_train_new[val_indexes],y_pred_temp)\n",
    "    y_folds = y_train_new[val_indexes]\n",
    "    score_false_negativ = 0\n",
    "    nb_positivs=0\n",
    "    for i in range(len(y_train[val_indexes])):\n",
    "        if y_folds[i][0]:\n",
    "            nb_positivs+=1\n",
    "            if not(y_pred_temp[i]):\n",
    "                score_false_negativ+=1\n",
    "    models_scores_val_SVC_new.append([model_temp,f1_score_temp,score_false_negativ/nb_positivs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[SVC(gamma='auto'), 0.788235294117647, 0.2717391304347826],\n",
       " [SVC(gamma='auto'), 0.6666666666666667, 0.41414141414141414],\n",
       " [SVC(gamma='auto'), 0.7710843373493976, 0.2808988764044944],\n",
       " [SVC(gamma='auto'), 0.6617647058823529, 0.4155844155844156],\n",
       " [SVC(gamma='auto'), 0.7734806629834253, 0.29292929292929293],\n",
       " [SVC(gamma='auto'), 0.7692307692307693, 0.29292929292929293],\n",
       " [SVC(gamma='auto'), 0.7717391304347825, 0.29],\n",
       " [SVC(gamma='auto'), 0.75, 0.25925925925925924],\n",
       " [SVC(gamma='auto'), 0.783132530120482, 0.3010752688172043],\n",
       " [SVC(gamma='auto'), 0.7534246575342465, 0.32926829268292684],\n",
       " [SVC(gamma='auto'), 0.7439024390243902, 0.3068181818181818],\n",
       " [SVC(gamma='auto'), 0.7473684210526315, 0.30392156862745096],\n",
       " [SVC(gamma='auto'), 0.781725888324873, 0.28703703703703703],\n",
       " [SVC(gamma='auto'), 0.8, 0.2808988764044944],\n",
       " [SVC(gamma='auto'), 0.7909604519774011, 0.29292929292929293],\n",
       " [SVC(gamma='auto'), 0.7748691099476439, 0.3018867924528302]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_scores_val_SVC_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.788235294117647\n",
      "Number of false negatives: 0.2717391304347826\n"
     ]
    }
   ],
   "source": [
    "model_svm_selected_new = models_scores_val_SVC_new[0][0]\n",
    "print(\"F1 score:\",models_scores_val_SVC_new[0][1])\n",
    "print(\"Number of false negatives:\",models_scores_val_SVC_new[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_scores_val_NN_new = []\n",
    "\n",
    "\n",
    "for train_indexes, val_indexes in kf.split(X_new_standardized):\n",
    "    model_temp = MLPClassifier(hidden_layer_sizes=(6,10,6,4), activation=\"relu\",max_iter=1000)\n",
    "    model_temp.fit(X_new_standardized[train_indexes], y_train_new[train_indexes].ravel())\n",
    "    y_pred_temp = model_temp.predict(X_new_standardized[val_indexes])\n",
    "    f1_score_temp = f1_score(y_train_new[val_indexes],y_pred_temp)\n",
    "    y_folds = y_train_new[val_indexes]\n",
    "    score_false_negativ = 0\n",
    "    nb_positivs=0\n",
    "    for i in range(len(y_train[val_indexes])):\n",
    "        if y_folds[i][0]:\n",
    "            nb_positivs+=1\n",
    "            if not(y_pred_temp[i]):\n",
    "                score_false_negativ+=1\n",
    "    models_scores_val_NN_new.append([model_temp,f1_score_temp,score_false_negativ/nb_positivs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.711111111111111,\n",
       "  0.30434782608695654],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6519337016574586,\n",
       "  0.40404040404040403],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7052023121387283,\n",
       "  0.3146067415730337],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6715328467153285,\n",
       "  0.4025974025974026],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7046632124352332,\n",
       "  0.31313131313131315],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.770053475935829,\n",
       "  0.2727272727272727],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7643979057591622,\n",
       "  0.27],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7450980392156864,\n",
       "  0.2962962962962963],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.689655172413793,\n",
       "  0.3548387096774194],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7189542483660131,\n",
       "  0.32926829268292684],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7388535031847134,\n",
       "  0.3409090909090909],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6984126984126984,\n",
       "  0.35294117647058826],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7282051282051282,\n",
       "  0.3425925925925926],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7045454545454545,\n",
       "  0.30337078651685395],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7513227513227514,\n",
       "  0.2828282828282828],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7192118226600984,\n",
       "  0.3113207547169811]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_scores_val_NN_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.770053475935829\n",
      "Number of false negatives: 0.2727272727272727\n"
     ]
    }
   ],
   "source": [
    "model_NN_selected_new = models_scores_val_NN_new[5][0]\n",
    "print(\"F1 score:\",models_scores_val_NN_new[5][1])\n",
    "print(\"Number of false negatives:\",models_scores_val_NN_new[5][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_scores_val_NN_PCA_new = []\n",
    "\n",
    "\n",
    "for train_indexes, val_indexes in kf.split(X_new_transformed):\n",
    "    model_temp = MLPClassifier(hidden_layer_sizes=(6,10,6,4), activation=\"relu\",max_iter=1000)\n",
    "    model_temp.fit(X_new_transformed[train_indexes], y_train_new[train_indexes].ravel())\n",
    "    y_pred_temp = model_temp.predict(X_new_transformed[val_indexes])\n",
    "    f1_score_temp = f1_score(y_train_new[val_indexes],y_pred_temp)\n",
    "    y_folds = y_train_new[val_indexes]\n",
    "    score_false_negativ = 0\n",
    "    nb_positivs=0\n",
    "    for i in range(len(y_train[val_indexes])):\n",
    "        if y_folds[i][0]:\n",
    "            nb_positivs+=1\n",
    "            if not(y_pred_temp[i]):\n",
    "                score_false_negativ+=1\n",
    "    models_scores_val_NN_PCA_new.append([model_temp,f1_score_temp,score_false_negativ/nb_positivs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7305389221556886,\n",
       "  0.33695652173913043],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6347305389221557,\n",
       "  0.46464646464646464],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.691358024691358,\n",
       "  0.3707865168539326],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6119402985074627,\n",
       "  0.4675324675324675],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7085714285714285,\n",
       "  0.37373737373737376],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7789473684210526,\n",
       "  0.25252525252525254],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7407407407407408,\n",
       "  0.3],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7044025157232704,\n",
       "  0.30864197530864196],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7393939393939394,\n",
       "  0.34408602150537637],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.6878980891719746,\n",
       "  0.34146341463414637],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7388535031847134,\n",
       "  0.3409090909090909],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.71875,\n",
       "  0.3235294117647059],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.78,\n",
       "  0.2777777777777778],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.703030303030303,\n",
       "  0.34831460674157305],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7292817679558011,\n",
       "  0.3333333333333333],\n",
       " [MLPClassifier(hidden_layer_sizes=(6, 10, 6, 4), max_iter=1000),\n",
       "  0.7474747474747474,\n",
       "  0.3018867924528302]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_scores_val_NN_PCA_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7789473684210526\n",
      "Number of false negatives: 0.25252525252525254\n"
     ]
    }
   ],
   "source": [
    "model_NN_PCA_selected_new = models_scores_val_NN_PCA_new[5][0]\n",
    "print(\"F1 score:\",models_scores_val_NN_PCA_new[5][1])\n",
    "print(\"Number of false negatives:\",models_scores_val_NN_PCA_new[5][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score for Logistic Regression with PCA transformation on X_test: 0.746268656716418 and the number of false negativ: 14\n",
      "f1_score for SVM on X_test: 0.7605633802816902 and the number of false negativ: 12\n",
      "f1_score for NN on X_test: 0.7142857142857142 and the number of false negativ: 14\n",
      "f1_score for NN with PCA transformation on X_test: 0.7397260273972601 and the number of false negativ: 12\n"
     ]
    }
   ],
   "source": [
    "X_test_standardized_new = scaler_2.transform(X_test_new)\n",
    "X_test_transformed_new = pca_2.transform(X_test_standardized_new)\n",
    "\n",
    "y_pred_X_test_log_rec_new = model_log_rec_selected_new.predict(X_test_transformed_new)\n",
    "y_pred_X_test_SVM_new = model_svm_selected_new.predict(X_test_standardized_new)\n",
    "y_pred_X_test_NN_PCA_new = model_NN_PCA_selected_new.predict(X_test_transformed_new)\n",
    "y_pred_X_test_NN_new = model_NN_selected_new.predict(X_test_standardized_new)\n",
    "\n",
    "f1_score_log_rec_new = f1_score(y_test_new.ravel(),y_pred_X_test_log_rec_new)\n",
    "score_false_negativ_log_rec_new = 0\n",
    "for i in range(len(y_test_new)):\n",
    "    if y_test_new[i] and not(y_pred_X_test_log_rec_new[i]):\n",
    "        score_false_negativ_log_rec_new+=1\n",
    "\n",
    "f1_score_SVM_new = f1_score(y_test_new.ravel(),y_pred_X_test_SVM_new)\n",
    "score_false_negativ_SVM_new = 0\n",
    "for i in range(len(y_test_new)):\n",
    "    if y_test_new[i] and not(y_pred_X_test_SVM_new[i]):\n",
    "        score_false_negativ_SVM_new+=1\n",
    "\n",
    "f1_score_NN_new = f1_score(y_test_new.ravel(),y_pred_X_test_NN_new)\n",
    "score_false_negativ_NN_new = 0\n",
    "for i in range(len(y_test_new)):\n",
    "    if y_test_new[i] and not(y_pred_X_test_NN_new[i]):\n",
    "        score_false_negativ_NN_new+=1\n",
    "\n",
    "f1_score_NN_PCA_new = f1_score(y_test_new.ravel(),y_pred_X_test_NN_PCA_new)\n",
    "score_false_negativ_NN_PCA_new = 0\n",
    "for i in range(len(y_test_new)):\n",
    "    if y_test_new[i] and not(y_pred_X_test_NN_PCA_new[i]):\n",
    "        score_false_negativ_NN_PCA_new+=1\n",
    "\n",
    "\n",
    "\n",
    "print(f\"f1_score for Logistic Regression with PCA transformation on X_test: {f1_score_log_rec_new} and the number of false negativ: {score_false_negativ_log_rec_new}\")\n",
    "print(f\"f1_score for SVM on X_test: {f1_score_SVM_new} and the number of false negativ: {score_false_negativ_SVM_new}\")\n",
    "print(f\"f1_score for NN on X_test: {f1_score_NN_new} and the number of false negativ: {score_false_negativ_NN_new}\")\n",
    "print(f\"f1_score for NN with PCA transformation on X_test: {f1_score_NN_PCA_new} and the number of false negativ: {score_false_negativ_NN_PCA_new}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly select the SVM model here to classify the micro-calcification as predicting cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Discussion about the results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are nice but no satisfying and can be improved:\n",
    "- In step 1, now that we know that NN with PCA seems to be a good choice to predict if a micro-calcification, we can play on hyperparameters and regularisation to improve the model. We could select them by choosing the hyperparameters minimizing the validation error.\n",
    "- In step 2, SVM seems to do the job. We could imagine playing with the C argument that is a Regularization parameter to improve the model, in the training phase, by selecting the C value that minimize the validation error.\n",
    "- In step 2, we could avoid a certain number of false negatives of the model, by taking \"an average\" when taking the decision of wheter a person has cancer : multiple micro-calcifications belong to one person, so if a majority of micro-calcifications predicts that the person has a cancer, we take the decision to state that the model predicts \"cancer\" for the patient. We could be even stricter by saying that if one micro-calcification of a person predicts cancer, we decide to state that the model predicts \"cancer\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning techniques allow us here to predict really quickly whether a person has cancer or not. If the models above are improved, they could really get into the action to diagnose breast cancer. But we should keep in mind that the database was quite small and another way to improve the model could be a kind of continuous learning (\"online learning\") where the model is still trained with new or updated data. Last but not least, even if ML techniques offer a good solution to predict breast cancer, we should not let them do all the jobs, and a human with its critical point of view should stay behind to double check the results : when selecting the models, I (a human) was selecting the models by doing a tradeoff bewteen a good f1 score while keeping the ratio of false negatives low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226\n",
    "* https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f2d51b4a2ffd6f5549f6061efbc8893eb6a35efcd896968c7578382b43818ef"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
